max_seq_length = 140
dataset_path = "./data/it_training_data"
load_in_4bit = true
early_stopping_patience = 3
early_stopping_threshold = 0.05

[ModelArgs]
r = 16 # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
lora_alpha = 16
lora_dropout = 0 # Supports any, but = 0 is optimized
bias = "none"    # Supports any, but = "none" is optimized
use_gradient_checkpointing = true
use_rslora = false

[TrainingArguments]
per_device_train_batch_size = 16
per_device_eval_batch_size = 2
gradient_accumulation_steps = 4
save_steps = "yes"
warmup_steps = 5
learning_rate = 2e-4
num_train_epochs = 2
fp16 = false
bf16 = true
logging_steps = 50
optim = "adamw_8bit"
weight_decay = 0.01
eval_steps = 50
evaluation_strategy = "no" # 'no' or 'steps' or 'epoch'
do_eval = true
lr_scheduler_type = "linear"
seed = 42
# report_to = "wandb"
output_dir = "outputs"